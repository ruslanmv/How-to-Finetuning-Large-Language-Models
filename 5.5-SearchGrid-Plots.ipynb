{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:50:31,967 - DEBUG - utilities - Config: datasets.path: c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n",
      "datasets.use_hf: false\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize False c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:50:32,485 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-59ea57fe03c7d0e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n",
      "2024-04-08 23:50:32,500 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-59ea57fe03c7d0e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyperparameters:\n",
      "learning_rate: 1e-05\n",
      "num_train_epochs: 1\n",
      "per_device_train_batch_size: 1\n",
      "optim: adafactor\n",
      "num_iterations: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3c0854fc4e460194892d7c20f5a4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8075, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 4.059, 'learning_rate': 9.970238095238096e-06, 'epoch': 0.01}\n",
      "{'loss': 4.6117, 'learning_rate': 9.940476190476192e-06, 'epoch': 0.01}\n",
      "{'loss': 3.897, 'learning_rate': 9.910714285714288e-06, 'epoch': 0.01}\n",
      "{'loss': 4.0117, 'learning_rate': 9.880952380952381e-06, 'epoch': 0.01}\n",
      "{'loss': 4.0519, 'learning_rate': 9.851190476190477e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2927, 'learning_rate': 9.821428571428573e-06, 'epoch': 0.02}\n",
      "{'loss': 3.7339, 'learning_rate': 9.791666666666666e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4974, 'learning_rate': 9.761904761904762e-06, 'epoch': 0.03}\n",
      "{'loss': 3.3106, 'learning_rate': 9.732142857142858e-06, 'epoch': 0.03}\n",
      "{'loss': 3.796, 'learning_rate': 9.702380952380953e-06, 'epoch': 0.03}\n",
      "{'loss': 3.3639, 'learning_rate': 9.672619047619049e-06, 'epoch': 0.04}\n",
      "{'loss': 3.4088, 'learning_rate': 9.642857142857144e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2516, 'learning_rate': 9.61309523809524e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3604, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.04}\n",
      "{'loss': 3.0163, 'learning_rate': 9.55357142857143e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3007, 'learning_rate': 9.523809523809525e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1068, 'learning_rate': 9.494047619047619e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3314, 'learning_rate': 9.464285714285714e-06, 'epoch': 0.06}\n",
      "{'loss': 2.8026, 'learning_rate': 9.43452380952381e-06, 'epoch': 0.06}\n",
      "{'loss': 2.8114, 'learning_rate': 9.404761904761905e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4086, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3829, 'learning_rate': 9.345238095238096e-06, 'epoch': 0.07}\n",
      "{'loss': 1.8713, 'learning_rate': 9.315476190476192e-06, 'epoch': 0.07}\n",
      "{'loss': 2.4715, 'learning_rate': 9.285714285714288e-06, 'epoch': 0.07}\n",
      "{'loss': 3.0779, 'learning_rate': 9.255952380952381e-06, 'epoch': 0.08}\n",
      "{'loss': 3.1865, 'learning_rate': 9.226190476190477e-06, 'epoch': 0.08}\n",
      "{'loss': 2.2997, 'learning_rate': 9.196428571428571e-06, 'epoch': 0.08}\n",
      "{'loss': 1.9368, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2455, 'learning_rate': 9.136904761904762e-06, 'epoch': 0.09}\n",
      "{'loss': 2.9447, 'learning_rate': 9.107142857142858e-06, 'epoch': 0.09}\n",
      "{'loss': 1.9661, 'learning_rate': 9.077380952380953e-06, 'epoch': 0.09}\n",
      "{'loss': 3.5607, 'learning_rate': 9.047619047619049e-06, 'epoch': 0.1}\n",
      "{'loss': 2.1228, 'learning_rate': 9.017857142857144e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4673, 'learning_rate': 8.98809523809524e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3014, 'learning_rate': 8.958333333333334e-06, 'epoch': 0.11}\n",
      "{'loss': 1.9767, 'learning_rate': 8.92857142857143e-06, 'epoch': 0.11}\n",
      "{'loss': 2.0574, 'learning_rate': 8.898809523809525e-06, 'epoch': 0.11}\n",
      "{'loss': 2.5112, 'learning_rate': 8.869047619047619e-06, 'epoch': 0.12}\n",
      "{'loss': 2.3741, 'learning_rate': 8.839285714285714e-06, 'epoch': 0.12}\n",
      "{'loss': 1.6681, 'learning_rate': 8.80952380952381e-06, 'epoch': 0.12}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from utilities import tokenize_and_split_data\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricsCollector(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to collect metrics during training.\n",
    "\n",
    "    This callback stores all the logs it receives during training in a list\n",
    "    called `metrics`. This list can then be used to plot training loss, learning rate,\n",
    "    and other metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Stores the logs received during training.\n",
    "\n",
    "        This method is called whenever the trainer logs information. It simply\n",
    "        appends the entire `logs` dictionary to the `metrics` list.\n",
    "\n",
    "        Args:\n",
    "          args: Arguments passed to the trainer.\n",
    "          state: State of the trainer.\n",
    "          control: Control object for the trainer.\n",
    "          logs: Dictionary containing the logged metrics. (optional)\n",
    "          **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        self.metrics.append(logs)\n",
    "\n",
    "\n",
    "def plot_loss(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the training loss from the collected metrics and saves the plot.\n",
    "\n",
    "    This function iterates through the `metrics` list and extracts the `loss` value\n",
    "    from each dictionary. It then filters out any entries where `loss` is missing\n",
    "    and plots the remaining values. The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    losses = [m.get('loss', None) for m in metrics]  # Use .get() to handle missing keys\n",
    "    non_none_losses = [loss for loss in losses if loss is not None]\n",
    "    plt.plot(non_none_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(os.path.join(output_dir, 'training_loss_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_learning_rate(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the learning rate from the collected metrics and saves the plot.\n",
    "\n",
    "    This function follows the same logic as `plot_loss` to extract and plot the\n",
    "    learning rate values from the `metrics` list, handling missing entries.\n",
    "    The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    learning_rates = [m.get('learning_rate', None) for m in metrics]\n",
    "    non_none_learning_rates = [lr for lr in learning_rates if lr is not None]\n",
    "    plt.plot(non_none_learning_rates)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.savefig(os.path.join(output_dir, 'learning_rate_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def find_best_hyperparameters():\n",
    "    model_name = \"EleutherAI/pythia-70m\"\n",
    "    use_hf = False\n",
    "    current_directory = os.getcwd()\n",
    "    folder_path = os.path.join(current_directory, \"content\")\n",
    "    dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "    dataset_path = os.path.join(folder_path, dataset_name)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    training_config = {\n",
    "        \"model\": {\n",
    "            \"pretrained_name\": model_name,\n",
    "            \"max_length\" : 2048\n",
    "        },\n",
    "        \"datasets\": {\n",
    "            \"use_hf\": use_hf,\n",
    "            \"path\": dataset_path\n",
    "        },\n",
    "        \"verbose\": True\n",
    "    }\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "    best_hyperparameters = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    hyperparameter_space = {\n",
    "        \"learning_rate\": [1e-05],\n",
    "        \"num_train_epochs\": [1],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "        \"num_iterations\": [1],\n",
    "    }\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    all_hyperparameters = list(itertools.product(*hyperparameter_space.values()))\n",
    "\n",
    "    for hyperparameter_values in all_hyperparameters:\n",
    "        hyperparameters = dict(zip(hyperparameter_space.keys(), hyperparameter_values))\n",
    "\n",
    "        # Print the current hyperparameters\n",
    "        print(\"Using hyperparameters:\")\n",
    "        for key, value in hyperparameters.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # Setup training_args with the current hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=hyperparameters[\"learning_rate\"],\n",
    "            num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "            per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "            output_dir=\"./results\",  # Provide a dummy output directory\n",
    "            overwrite_output_dir=False,\n",
    "            disable_tqdm=False,\n",
    "            eval_steps=120,\n",
    "            save_steps=120,\n",
    "            warmup_steps=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            optim=hyperparameters[\"optim\"],\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=False,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        base_model.to(device)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=base_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset\n",
    "        )\n",
    "\n",
    "        metrics_collector = MetricsCollector()\n",
    "        trainer.add_callback(metrics_collector)\n",
    "\n",
    "        training_output = trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        if eval_results[\"eval_loss\"] < best_loss:\n",
    "            best_loss = eval_results[\"eval_loss\"]\n",
    "            best_hyperparameters = hyperparameters\n",
    "        \n",
    "\n",
    "        output_dir = os.path.join(current_directory, \"SearchGrid\")\n",
    "\n",
    "        plot_loss(metrics_collector.metrics, output_dir)\n",
    "        plot_learning_rate(metrics_collector.metrics, output_dir)\n",
    "\n",
    "    return best_hyperparameters, best_loss\n",
    "\n",
    "\n",
    "best_hyperparameters, best_loss = find_best_hyperparameters()\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
