{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from utilities import tokenize_and_split_data\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricsCollector(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to collect metrics during training.\n",
    "\n",
    "    This callback stores all the logs it receives during training in a list\n",
    "    called `metrics`. This list can then be used to plot training loss, learning rate,\n",
    "    and other metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Stores the logs received during training.\n",
    "\n",
    "        This method is called whenever the trainer logs information. It simply\n",
    "        appends the entire `logs` dictionary to the `metrics` list.\n",
    "\n",
    "        Args:\n",
    "          args: Arguments passed to the trainer.\n",
    "          state: State of the trainer.\n",
    "          control: Control object for the trainer.\n",
    "          logs: Dictionary containing the logged metrics. (optional)\n",
    "          **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        self.metrics.append(logs)\n",
    "\n",
    "\n",
    "def plot_loss(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the training loss from the collected metrics and saves the plot.\n",
    "\n",
    "    This function iterates through the `metrics` list and extracts the `loss` value\n",
    "    from each dictionary. It then filters out any entries where `loss` is missing\n",
    "    and plots the remaining values. The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    losses = [m.get('loss', None) for m in metrics]  # Use .get() to handle missing keys\n",
    "    non_none_losses = [loss for loss in losses if loss is not None]\n",
    "    plt.plot(non_none_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(os.path.join(output_dir, 'training_loss_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_learning_rate(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the learning rate from the collected metrics and saves the plot.\n",
    "\n",
    "    This function follows the same logic as `plot_loss` to extract and plot the\n",
    "    learning rate values from the `metrics` list, handling missing entries.\n",
    "    The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    learning_rates = [m.get('learning_rate', None) for m in metrics]\n",
    "    non_none_learning_rates = [lr for lr in learning_rates if lr is not None]\n",
    "    plt.plot(non_none_learning_rates)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.savefig(os.path.join(output_dir, 'learning_rate_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def find_best_hyperparameters():\n",
    "    model_name = \"EleutherAI/pythia-70m\"\n",
    "    use_hf = False\n",
    "    current_directory = os.getcwd()\n",
    "    folder_path = os.path.join(current_directory, \"content\")\n",
    "    dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "    dataset_path = os.path.join(folder_path, dataset_name)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    training_config = {\n",
    "        \"model\": {\n",
    "            \"pretrained_name\": model_name,\n",
    "            \"max_length\" : 2048\n",
    "        },\n",
    "        \"datasets\": {\n",
    "            \"use_hf\": use_hf,\n",
    "            \"path\": dataset_path\n",
    "        },\n",
    "        \"verbose\": True\n",
    "    }\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "    best_hyperparameters = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    hyperparameter_space = {\n",
    "        \"learning_rate\": [1e-5],\n",
    "        \"num_train_epochs\": [1],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "    }\n",
    "\n",
    "    num_iterations = 1\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        hyperparameters = {\n",
    "            \"learning_rate\": random.choice(hyperparameter_space[\"learning_rate\"]),\n",
    "            \"num_train_epochs\": random.choice(hyperparameter_space[\"num_train_epochs\"]),\n",
    "            \"per_device_train_batch_size\": random.choice(hyperparameter_space[\"per_device_train_batch_size\"]),\n",
    "            \"optim\": random.choice(hyperparameter_space[\"optim\"]),\n",
    "        }\n",
    "\n",
    "        output_dir = os.path.join(current_directory, \"best_fit\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=hyperparameters[\"learning_rate\"],\n",
    "            num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "            per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=False,\n",
    "            disable_tqdm=False,\n",
    "            eval_steps=120,\n",
    "            save_steps=120,\n",
    "            warmup_steps=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            optim=hyperparameters[\"optim\"],\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=False,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        base_model.to(device)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=base_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset\n",
    "        )\n",
    "\n",
    "        metrics_collector = MetricsCollector()\n",
    "        trainer.add_callback(metrics_collector)\n",
    "\n",
    "        training_output = trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        if eval_results[\"eval_loss\"] < best_loss:\n",
    "            best_loss = eval_results[\"eval_loss\"]\n",
    "            best_hyperparameters = hyperparameters\n",
    "\n",
    "        plot_loss(metrics_collector.metrics, output_dir)\n",
    "        plot_learning_rate(metrics_collector.metrics, output_dir)\n",
    "\n",
    "    return best_hyperparameters, best_loss\n",
    "\n",
    "\n",
    "best_hyperparameters, best_loss = find_best_hyperparameters()\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
