{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:50:31,967 - DEBUG - utilities - Config: datasets.path: c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n",
      "datasets.use_hf: false\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize False c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:50:32,485 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-59ea57fe03c7d0e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n",
      "2024-04-08 23:50:32,500 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-59ea57fe03c7d0e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyperparameters:\n",
      "learning_rate: 1e-05\n",
      "num_train_epochs: 1\n",
      "per_device_train_batch_size: 1\n",
      "optim: adafactor\n",
      "num_iterations: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3c0854fc4e460194892d7c20f5a4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8075, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 4.059, 'learning_rate': 9.970238095238096e-06, 'epoch': 0.01}\n",
      "{'loss': 4.6117, 'learning_rate': 9.940476190476192e-06, 'epoch': 0.01}\n",
      "{'loss': 3.897, 'learning_rate': 9.910714285714288e-06, 'epoch': 0.01}\n",
      "{'loss': 4.0117, 'learning_rate': 9.880952380952381e-06, 'epoch': 0.01}\n",
      "{'loss': 4.0519, 'learning_rate': 9.851190476190477e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2927, 'learning_rate': 9.821428571428573e-06, 'epoch': 0.02}\n",
      "{'loss': 3.7339, 'learning_rate': 9.791666666666666e-06, 'epoch': 0.02}\n",
      "{'loss': 3.4974, 'learning_rate': 9.761904761904762e-06, 'epoch': 0.03}\n",
      "{'loss': 3.3106, 'learning_rate': 9.732142857142858e-06, 'epoch': 0.03}\n",
      "{'loss': 3.796, 'learning_rate': 9.702380952380953e-06, 'epoch': 0.03}\n",
      "{'loss': 3.3639, 'learning_rate': 9.672619047619049e-06, 'epoch': 0.04}\n",
      "{'loss': 3.4088, 'learning_rate': 9.642857142857144e-06, 'epoch': 0.04}\n",
      "{'loss': 3.2516, 'learning_rate': 9.61309523809524e-06, 'epoch': 0.04}\n",
      "{'loss': 3.3604, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.04}\n",
      "{'loss': 3.0163, 'learning_rate': 9.55357142857143e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3007, 'learning_rate': 9.523809523809525e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1068, 'learning_rate': 9.494047619047619e-06, 'epoch': 0.05}\n",
      "{'loss': 3.3314, 'learning_rate': 9.464285714285714e-06, 'epoch': 0.06}\n",
      "{'loss': 2.8026, 'learning_rate': 9.43452380952381e-06, 'epoch': 0.06}\n",
      "{'loss': 2.8114, 'learning_rate': 9.404761904761905e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4086, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3829, 'learning_rate': 9.345238095238096e-06, 'epoch': 0.07}\n",
      "{'loss': 1.8713, 'learning_rate': 9.315476190476192e-06, 'epoch': 0.07}\n",
      "{'loss': 2.4715, 'learning_rate': 9.285714285714288e-06, 'epoch': 0.07}\n",
      "{'loss': 3.0779, 'learning_rate': 9.255952380952381e-06, 'epoch': 0.08}\n",
      "{'loss': 3.1865, 'learning_rate': 9.226190476190477e-06, 'epoch': 0.08}\n",
      "{'loss': 2.2997, 'learning_rate': 9.196428571428571e-06, 'epoch': 0.08}\n",
      "{'loss': 1.9368, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2455, 'learning_rate': 9.136904761904762e-06, 'epoch': 0.09}\n",
      "{'loss': 2.9447, 'learning_rate': 9.107142857142858e-06, 'epoch': 0.09}\n",
      "{'loss': 1.9661, 'learning_rate': 9.077380952380953e-06, 'epoch': 0.09}\n",
      "{'loss': 3.5607, 'learning_rate': 9.047619047619049e-06, 'epoch': 0.1}\n",
      "{'loss': 2.1228, 'learning_rate': 9.017857142857144e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4673, 'learning_rate': 8.98809523809524e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3014, 'learning_rate': 8.958333333333334e-06, 'epoch': 0.11}\n",
      "{'loss': 1.9767, 'learning_rate': 8.92857142857143e-06, 'epoch': 0.11}\n",
      "{'loss': 2.0574, 'learning_rate': 8.898809523809525e-06, 'epoch': 0.11}\n",
      "{'loss': 2.5112, 'learning_rate': 8.869047619047619e-06, 'epoch': 0.12}\n",
      "{'loss': 2.3741, 'learning_rate': 8.839285714285714e-06, 'epoch': 0.12}\n",
      "{'loss': 1.6681, 'learning_rate': 8.80952380952381e-06, 'epoch': 0.12}\n",
      "{'loss': 1.9985, 'learning_rate': 8.779761904761905e-06, 'epoch': 0.12}\n",
      "{'loss': 3.5802, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.13}\n",
      "{'loss': 1.8183, 'learning_rate': 8.720238095238096e-06, 'epoch': 0.13}\n",
      "{'loss': 2.6301, 'learning_rate': 8.690476190476192e-06, 'epoch': 0.13}\n",
      "{'loss': 3.5032, 'learning_rate': 8.660714285714286e-06, 'epoch': 0.14}\n",
      "{'loss': 2.7681, 'learning_rate': 8.630952380952381e-06, 'epoch': 0.14}\n",
      "{'loss': 3.778, 'learning_rate': 8.601190476190477e-06, 'epoch': 0.14}\n",
      "{'loss': 2.6896, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.15}\n",
      "{'loss': 1.0452, 'learning_rate': 8.541666666666666e-06, 'epoch': 0.15}\n",
      "{'loss': 2.566, 'learning_rate': 8.511904761904762e-06, 'epoch': 0.15}\n",
      "{'loss': 3.0587, 'learning_rate': 8.482142857142858e-06, 'epoch': 0.15}\n",
      "{'loss': 2.9765, 'learning_rate': 8.452380952380953e-06, 'epoch': 0.16}\n",
      "{'loss': 2.9002, 'learning_rate': 8.422619047619049e-06, 'epoch': 0.16}\n",
      "{'loss': 2.5665, 'learning_rate': 8.392857142857144e-06, 'epoch': 0.16}\n",
      "{'loss': 2.4826, 'learning_rate': 8.36309523809524e-06, 'epoch': 0.17}\n",
      "{'loss': 2.7156, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.17}\n",
      "{'loss': 3.5829, 'learning_rate': 8.30357142857143e-06, 'epoch': 0.17}\n",
      "{'loss': 2.4452, 'learning_rate': 8.273809523809523e-06, 'epoch': 0.17}\n",
      "{'loss': 2.5979, 'learning_rate': 8.244047619047619e-06, 'epoch': 0.18}\n",
      "{'loss': 0.3027, 'learning_rate': 8.214285714285714e-06, 'epoch': 0.18}\n",
      "{'loss': 3.5425, 'learning_rate': 8.18452380952381e-06, 'epoch': 0.18}\n",
      "{'loss': 2.1496, 'learning_rate': 8.154761904761905e-06, 'epoch': 0.19}\n",
      "{'loss': 1.3662, 'learning_rate': 8.125000000000001e-06, 'epoch': 0.19}\n",
      "{'loss': 3.441, 'learning_rate': 8.095238095238097e-06, 'epoch': 0.19}\n",
      "{'loss': 2.8201, 'learning_rate': 8.065476190476192e-06, 'epoch': 0.2}\n",
      "{'loss': 2.5735, 'learning_rate': 8.035714285714286e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3741, 'learning_rate': 8.005952380952382e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3853, 'learning_rate': 7.976190476190477e-06, 'epoch': 0.2}\n",
      "{'loss': 2.2675, 'learning_rate': 7.946428571428571e-06, 'epoch': 0.21}\n",
      "{'loss': 2.0727, 'learning_rate': 7.916666666666667e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3447, 'learning_rate': 7.886904761904762e-06, 'epoch': 0.21}\n",
      "{'loss': 2.0226, 'learning_rate': 7.857142857142858e-06, 'epoch': 0.22}\n",
      "{'loss': 2.3415, 'learning_rate': 7.827380952380953e-06, 'epoch': 0.22}\n",
      "{'loss': 3.107, 'learning_rate': 7.797619047619049e-06, 'epoch': 0.22}\n",
      "{'loss': 3.5644, 'learning_rate': 7.767857142857144e-06, 'epoch': 0.23}\n",
      "{'loss': 1.6502, 'learning_rate': 7.738095238095238e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3508, 'learning_rate': 7.708333333333334e-06, 'epoch': 0.23}\n",
      "{'loss': 2.8922, 'learning_rate': 7.67857142857143e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2592, 'learning_rate': 7.648809523809523e-06, 'epoch': 0.24}\n",
      "{'loss': 2.212, 'learning_rate': 7.61904761904762e-06, 'epoch': 0.24}\n",
      "{'loss': 2.0129, 'learning_rate': 7.589285714285714e-06, 'epoch': 0.24}\n",
      "{'loss': 2.6586, 'learning_rate': 7.55952380952381e-06, 'epoch': 0.25}\n",
      "{'loss': 1.4965, 'learning_rate': 7.5297619047619055e-06, 'epoch': 0.25}\n",
      "{'loss': 1.2509, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 1.995, 'learning_rate': 7.470238095238096e-06, 'epoch': 0.25}\n",
      "{'loss': 2.5857, 'learning_rate': 7.440476190476191e-06, 'epoch': 0.26}\n",
      "{'loss': 1.0679, 'learning_rate': 7.410714285714287e-06, 'epoch': 0.26}\n",
      "{'loss': 2.7469, 'learning_rate': 7.380952380952382e-06, 'epoch': 0.26}\n",
      "{'loss': 3.6456, 'learning_rate': 7.351190476190477e-06, 'epoch': 0.27}\n",
      "{'loss': 2.4628, 'learning_rate': 7.321428571428572e-06, 'epoch': 0.27}\n",
      "{'loss': 3.5103, 'learning_rate': 7.291666666666667e-06, 'epoch': 0.27}\n",
      "{'loss': 3.5475, 'learning_rate': 7.261904761904762e-06, 'epoch': 0.28}\n",
      "{'loss': 1.921, 'learning_rate': 7.232142857142858e-06, 'epoch': 0.28}\n",
      "{'loss': 1.6817, 'learning_rate': 7.202380952380953e-06, 'epoch': 0.28}\n",
      "{'loss': 2.5971, 'learning_rate': 7.172619047619048e-06, 'epoch': 0.28}\n",
      "{'loss': 2.1846, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.29}\n",
      "{'loss': 2.9005, 'learning_rate': 7.113095238095239e-06, 'epoch': 0.29}\n",
      "{'loss': 2.334, 'learning_rate': 7.083333333333335e-06, 'epoch': 0.29}\n",
      "{'loss': 0.9426, 'learning_rate': 7.053571428571429e-06, 'epoch': 0.3}\n",
      "{'loss': 2.1596, 'learning_rate': 7.023809523809524e-06, 'epoch': 0.3}\n",
      "{'loss': 1.8844, 'learning_rate': 6.994047619047619e-06, 'epoch': 0.3}\n",
      "{'loss': 0.9951, 'learning_rate': 6.964285714285714e-06, 'epoch': 0.31}\n",
      "{'loss': 3.0383, 'learning_rate': 6.93452380952381e-06, 'epoch': 0.31}\n",
      "{'loss': 2.6302, 'learning_rate': 6.9047619047619055e-06, 'epoch': 0.31}\n",
      "{'loss': 2.7583, 'learning_rate': 6.875e-06, 'epoch': 0.31}\n",
      "{'loss': 1.5221, 'learning_rate': 6.845238095238096e-06, 'epoch': 0.32}\n",
      "{'loss': 2.3842, 'learning_rate': 6.815476190476191e-06, 'epoch': 0.32}\n",
      "{'loss': 2.3448, 'learning_rate': 6.785714285714287e-06, 'epoch': 0.32}\n",
      "{'loss': 2.8368, 'learning_rate': 6.755952380952382e-06, 'epoch': 0.33}\n",
      "{'loss': 2.4214, 'learning_rate': 6.726190476190477e-06, 'epoch': 0.33}\n",
      "{'loss': 2.9601, 'learning_rate': 6.696428571428571e-06, 'epoch': 0.33}\n",
      "{'loss': 2.508, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.33}\n",
      "{'loss': 2.2544, 'learning_rate': 6.636904761904762e-06, 'epoch': 0.34}\n",
      "{'loss': 2.6423, 'learning_rate': 6.607142857142858e-06, 'epoch': 0.34}\n",
      "{'loss': 2.8409, 'learning_rate': 6.5773809523809525e-06, 'epoch': 0.34}\n",
      "{'loss': 2.2786, 'learning_rate': 6.547619047619048e-06, 'epoch': 0.35}\n",
      "{'loss': 2.4441, 'learning_rate': 6.517857142857144e-06, 'epoch': 0.35}\n",
      "{'loss': 2.3535, 'learning_rate': 6.488095238095239e-06, 'epoch': 0.35}\n",
      "{'loss': 1.9456, 'learning_rate': 6.458333333333334e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed1f40283764861b28183744018d27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2275211811065674, 'eval_runtime': 17.2312, 'eval_samples_per_second': 8.705, 'eval_steps_per_second': 8.705, 'epoch': 0.36}\n",
      "{'loss': 1.3276, 'learning_rate': 6.4285714285714295e-06, 'epoch': 0.36}\n",
      "{'loss': 2.7252, 'learning_rate': 6.398809523809524e-06, 'epoch': 0.36}\n",
      "{'loss': 2.7075, 'learning_rate': 6.369047619047619e-06, 'epoch': 0.36}\n",
      "{'loss': 1.9005, 'learning_rate': 6.3392857142857145e-06, 'epoch': 0.37}\n",
      "{'loss': 3.3708, 'learning_rate': 6.30952380952381e-06, 'epoch': 0.37}\n",
      "{'loss': 1.1787, 'learning_rate': 6.279761904761906e-06, 'epoch': 0.37}\n",
      "{'loss': 1.9572, 'learning_rate': 6.25e-06, 'epoch': 0.38}\n",
      "{'loss': 1.494, 'learning_rate': 6.220238095238096e-06, 'epoch': 0.38}\n",
      "{'loss': 2.0389, 'learning_rate': 6.1904761904761914e-06, 'epoch': 0.38}\n",
      "{'loss': 2.2449, 'learning_rate': 6.160714285714286e-06, 'epoch': 0.39}\n",
      "{'loss': 2.406, 'learning_rate': 6.130952380952382e-06, 'epoch': 0.39}\n",
      "{'loss': 3.184, 'learning_rate': 6.101190476190477e-06, 'epoch': 0.39}\n",
      "{'loss': 2.5001, 'learning_rate': 6.071428571428571e-06, 'epoch': 0.39}\n",
      "{'loss': 2.6609, 'learning_rate': 6.041666666666667e-06, 'epoch': 0.4}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from utilities import tokenize_and_split_data\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricsCollector(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to collect metrics during training.\n",
    "\n",
    "    This callback stores all the logs it receives during training in a list\n",
    "    called `metrics`. This list can then be used to plot training loss, learning rate,\n",
    "    and other metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Stores the logs received during training.\n",
    "\n",
    "        This method is called whenever the trainer logs information. It simply\n",
    "        appends the entire `logs` dictionary to the `metrics` list.\n",
    "\n",
    "        Args:\n",
    "          args: Arguments passed to the trainer.\n",
    "          state: State of the trainer.\n",
    "          control: Control object for the trainer.\n",
    "          logs: Dictionary containing the logged metrics. (optional)\n",
    "          **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        self.metrics.append(logs)\n",
    "\n",
    "\n",
    "def plot_loss(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the training loss from the collected metrics and saves the plot.\n",
    "\n",
    "    This function iterates through the `metrics` list and extracts the `loss` value\n",
    "    from each dictionary. It then filters out any entries where `loss` is missing\n",
    "    and plots the remaining values. The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    losses = [m.get('loss', None) for m in metrics]  # Use .get() to handle missing keys\n",
    "    non_none_losses = [loss for loss in losses if loss is not None]\n",
    "    plt.plot(non_none_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig(os.path.join(output_dir, 'training_loss_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_learning_rate(metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the learning rate from the collected metrics and saves the plot.\n",
    "\n",
    "    This function follows the same logic as `plot_loss` to extract and plot the\n",
    "    learning rate values from the `metrics` list, handling missing entries.\n",
    "    The plot is saved in the specified `output_dir`.\n",
    "\n",
    "    Args:\n",
    "      metrics: List of dictionaries containing training logs.\n",
    "      output_dir: Directory to save the plot.\n",
    "    \"\"\"\n",
    "    learning_rates = [m.get('learning_rate', None) for m in metrics]\n",
    "    non_none_learning_rates = [lr for lr in learning_rates if lr is not None]\n",
    "    plt.plot(non_none_learning_rates)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.savefig(os.path.join(output_dir, 'learning_rate_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def find_best_hyperparameters():\n",
    "    model_name = \"EleutherAI/pythia-70m\"\n",
    "    use_hf = False\n",
    "    current_directory = os.getcwd()\n",
    "    folder_path = os.path.join(current_directory, \"content\")\n",
    "    dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "    dataset_path = os.path.join(folder_path, dataset_name)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    training_config = {\n",
    "        \"model\": {\n",
    "            \"pretrained_name\": model_name,\n",
    "            \"max_length\" : 2048\n",
    "        },\n",
    "        \"datasets\": {\n",
    "            \"use_hf\": use_hf,\n",
    "            \"path\": dataset_path\n",
    "        },\n",
    "        \"verbose\": True\n",
    "    }\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "    best_hyperparameters = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    hyperparameter_space = {\n",
    "        \"learning_rate\": [1e-05],\n",
    "        \"num_train_epochs\": [1],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "        \"num_iterations\": [1],\n",
    "    }\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    all_hyperparameters = list(itertools.product(*hyperparameter_space.values()))\n",
    "\n",
    "    for hyperparameter_values in all_hyperparameters:\n",
    "        hyperparameters = dict(zip(hyperparameter_space.keys(), hyperparameter_values))\n",
    "\n",
    "        # Print the current hyperparameters\n",
    "        print(\"Using hyperparameters:\")\n",
    "        for key, value in hyperparameters.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # Setup training_args with the current hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            learning_rate=hyperparameters[\"learning_rate\"],\n",
    "            num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "            per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "            output_dir=\"./results\",  # Provide a dummy output directory\n",
    "            overwrite_output_dir=False,\n",
    "            disable_tqdm=False,\n",
    "            eval_steps=120,\n",
    "            save_steps=120,\n",
    "            warmup_steps=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            optim=hyperparameters[\"optim\"],\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=False,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        base_model.to(device)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=base_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset\n",
    "        )\n",
    "\n",
    "        metrics_collector = MetricsCollector()\n",
    "        trainer.add_callback(metrics_collector)\n",
    "\n",
    "        training_output = trainer.train()\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        if eval_results[\"eval_loss\"] < best_loss:\n",
    "            best_loss = eval_results[\"eval_loss\"]\n",
    "            best_hyperparameters = hyperparameters\n",
    "        \n",
    "\n",
    "        output_dir = os.path.join(current_directory, \"SearchGrid\")\n",
    "\n",
    "        plot_loss(metrics_collector.metrics, output_dir)\n",
    "        plot_learning_rate(metrics_collector.metrics, output_dir)\n",
    "\n",
    "    return best_hyperparameters, best_loss\n",
    "\n",
    "\n",
    "best_hyperparameters, best_loss = find_best_hyperparameters()\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
