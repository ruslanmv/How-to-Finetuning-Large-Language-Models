{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import itertools\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-160m\"\n",
    "#model_name = \"EleutherAI/pythia-70m\"\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "# Join the folder path\n",
    "folder_path = os.path.join(current_directory, \"content\")\n",
    "dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "dataset_path = os.path.join(folder_path, dataset_name)\n",
    "#dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def cosine_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two strings using the Bag-of-Words model.\n",
    "\n",
    "    Args:\n",
    "        str1: The first string.\n",
    "        str2: The second string.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the cosine similarity between the two strings.\n",
    "    \"\"\"\n",
    "    # Tokenize the strings\n",
    "    tokens1 = str1.split()\n",
    "    tokens2 = str2.split()\n",
    "\n",
    "    # Create bag of words for each string\n",
    "    bow1 = Counter(tokens1)\n",
    "    bow2 = Counter(tokens2)\n",
    "\n",
    "    # Get the set of all unique words\n",
    "    all_words = set(bow1.keys()).union(set(bow2.keys()))\n",
    "\n",
    "    # Compute dot product\n",
    "    dot_product = sum(bow1[word] * bow2[word] for word in all_words)\n",
    "\n",
    "    # Compute magnitudes\n",
    "    magnitude1 = math.sqrt(sum(bow1[word] ** 2 for word in all_words))\n",
    "    magnitude2 = math.sqrt(sum(bow2[word] ** 2 for word in all_words))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_new(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=1000):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "      text,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "\n",
    "  # Fix: Mask all padding tokens, including the first element\n",
    "  attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "      input_ids.to(device),\n",
    "      max_length=max_output_tokens,\n",
    "      attention_mask=attention_mask,\n",
    "      pad_token_id=tokenizer.eos_token_id  # Set pad token\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "  return generated_text_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import shutil\n",
    "def train_model(hyperparameters, delete=False, testing=False):\n",
    "  max_steps = hyperparameters[\"max_steps\"]\n",
    "\n",
    "\n",
    "  # Convert hyperparameter values to integers and add them to the string\n",
    "  hyperparameter_str = '_'.join(str(int(value)) if isinstance(value, (int, float)) else value for value in hyperparameters.values())\n",
    "  # Create the trained_model_name variable\n",
    "  trained_model_name = f\"ai_medical_{hyperparameter_str}\"\n",
    "\n",
    "  #trained_model_name = f\"ai_medical_{max_steps}_steps\"\n",
    "  output_dir = trained_model_name\n",
    "  training_args = TrainingArguments(\n",
    "    # Learning rate\n",
    "    learning_rate=hyperparameters[\"learning_rate\"],\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overrides num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "\n",
    "    # Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # Other arguments\n",
    "    overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "    disable_tqdm=False, # Disable progress bars\n",
    "    eval_steps=120, # Number of update steps between two evaluations\n",
    "    save_steps=120, # After # steps model is saved\n",
    "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=hyperparameters[\"optim\"],\n",
    "    gradient_accumulation_steps = hyperparameters['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=False,\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    "  )\n",
    "  base_model.to(device)\n",
    "  model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "      {\n",
    "        \"input_ids\": torch.zeros(\n",
    "            (1, training_config[\"model\"][\"max_length\"])\n",
    "        )\n",
    "      }\n",
    "    )\n",
    "    * training_args.gradient_accumulation_steps\n",
    "  )\n",
    "\n",
    "  #print(base_model)\n",
    "  print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "  print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "  training_output = trainer.train()\n",
    "  # Evaluate the model\n",
    "  eval_results = trainer.evaluate()\n",
    "\n",
    "  # Adding Evaluation \n",
    "  save_dir = f'{output_dir}'\n",
    "  trainer.save_model(save_dir)\n",
    "  print(\"Saved model to:\", save_dir)\n",
    "  finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "  finetuned_slightly_model.to(device)\n",
    "  test_question = test_dataset[0]['question']\n",
    "  print(\"Question input (test):\", test_question)\n",
    "  predicted_answer=inference_new(test_question, finetuned_slightly_model, tokenizer)\n",
    "  print(\"Finetuned slightly model's answer: \")\n",
    "  print(predicted_answer) \n",
    "  test_answer = test_dataset[0]['answer']\n",
    "  print(\"Target answer output (test):\", test_answer)\n",
    "  metric_cosine_similarity=cosine_similarity(test_answer, predicted_answer)\n",
    "  print(\"Cosine Similarity:\", metric_cosine_similarity)\n",
    "  # Deleting the folder to save space\n",
    "  clear_output()\n",
    "  if delete:\n",
    "    shutil.rmtree(save_dir)\n",
    "    print(\"Deleted model folder:\", save_dir)\n",
    "  if testing:\n",
    "    return eval_results, training_output, metric_cosine_similarity,test_question,test_answer,predicted_answer\n",
    "\n",
    "  else:\n",
    "    return eval_results, training_output, metric_cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'learning_rate': 1e-06,\n",
    "'num_train_epochs': 1,\n",
    "'per_device_train_batch_size': 1,\n",
    "'optim': 'adafactor',\n",
    "'num_iterations': 1,\n",
    "'max_steps':3,\n",
    "'gradient_accumulation_steps':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results, training_output, metric_cosine_similarity,test_question,test_answer,predicted_answer =train_model(hyperparameters, delete=True, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from ai-medical-chatbot: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "#print(inference_new(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "    best_hyperparameters = None\n",
    "    best_loss = float('inf')\n",
    "    # Lists to store data\n",
    "    hyperparameters_list = []\n",
    "    eval_results_list = []\n",
    "    training_output_list = []\n",
    "    cosine_similarity_list = []\n",
    "\n",
    "    test_question_list = []\n",
    "    test_answer_list = []\n",
    "    predicted_answer_list = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    hyperparameter_space = {\n",
    "        \"learning_rate\": [1e-6, 1e-5, 1e-4],\n",
    "        \"num_train_epochs\": [1,5,10,20],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "        \"max_steps\": [3],\n",
    "        \"gradient_accumulation_steps\": [3],\n",
    "    }\n",
    "    # Generate all combinations of hyperparameters\n",
    "    all_hyperparameters = list(itertools.product(*hyperparameter_space.values()))\n",
    "\n",
    "    # Assuming all_hyperparameters is a list of hyperparameter combinations\n",
    "    for hyperparameter_values in tqdm(all_hyperparameters):\n",
    "        hyperparameters = dict(zip(hyperparameter_space.keys(), hyperparameter_values))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        # Print the current hyperparameters\n",
    "        print(\"Using hyperparameters:\")\n",
    "        for key, value in hyperparameters.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        eval_results, training_output, metric_cosine_similarity, test_question, test_answer, predicted_answer = train_model(hyperparameters,delete=True,testing=True)\n",
    "        \n",
    "        # Append data to lists\n",
    "        hyperparameters_list.append(hyperparameters)\n",
    "        eval_results_list.append(eval_results)\n",
    "        training_output_list.append(training_output)\n",
    "        cosine_similarity_list.append(metric_cosine_similarity)\n",
    "\n",
    "        test_question_list.append(test_question)\n",
    "        test_answer_list.append(test_answer)\n",
    "        predicted_answer_list.append(predicted_answer)\n",
    "\n",
    "        # Check if this set of hyperparameters gives better results\n",
    "        if eval_results[\"eval_loss\"] < best_loss:\n",
    "                best_loss = eval_results[\"eval_loss\"]\n",
    "                best_hyperparameters = hyperparameters\n",
    "        clear_output()\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'Question':test_question_list,\n",
    "        'Answer':test_answer_list,\n",
    "        'Prediction':predicted_answer_list,\n",
    "        'Hyperparameters': hyperparameters_list,\n",
    "        'Evaluation Results': eval_results_list,\n",
    "        'Training Output': training_output_list,\n",
    "        'Cosine Similarity': cosine_similarity_list\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return best_hyperparameters, best_loss, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to find the best hyperparameters\n",
    "best_hyperparameters, best_loss ,df = find_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'eval_loss' inside the 'Evaluation Results' column\n",
    "df_sorted = df.sort_values(by='Evaluation Results', \n",
    "                           key=lambda x: x.apply(lambda d: d['eval_loss']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Cosine Similarity' from largest to smallest\n",
    "df_cos = df.sort_values(by='Cosine Similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos[[\"Answer\",\"Prediction\",\"Cosine Similarity\",\"Evaluation Results\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
