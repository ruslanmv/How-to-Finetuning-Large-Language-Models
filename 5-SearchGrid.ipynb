{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-160m\"\n",
    "#model_name = \"EleutherAI/pythia-70m\"\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "# Join the folder path\n",
    "folder_path = os.path.join(current_directory, \"content\")\n",
    "dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "dataset_path = os.path.join(folder_path, dataset_name)\n",
    "#dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hyperparameters):\n",
    "  max_steps = hyperparameters[\"max_steps\"]\n",
    "  trained_model_name = f\"ai_medical_{max_steps}_steps\"\n",
    "  output_dir = trained_model_name\n",
    "  training_args = TrainingArguments(\n",
    "    # Learning rate\n",
    "    learning_rate=hyperparameters[\"learning_rate\"],\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overrides num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "\n",
    "    # Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # Other arguments\n",
    "    overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "    disable_tqdm=False, # Disable progress bars\n",
    "    eval_steps=120, # Number of update steps between two evaluations\n",
    "    save_steps=120, # After # steps model is saved\n",
    "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=hyperparameters[\"optim\"],\n",
    "    gradient_accumulation_steps = hyperparameters['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=False,\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    "  )\n",
    "  base_model.to(device)\n",
    "  model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "      {\n",
    "        \"input_ids\": torch.zeros(\n",
    "            (1, training_config[\"model\"][\"max_length\"])\n",
    "        )\n",
    "      }\n",
    "    )\n",
    "    * training_args.gradient_accumulation_steps\n",
    "  )\n",
    "\n",
    "  #print(base_model)\n",
    "  print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "  print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "  training_output = trainer.train()\n",
    "  # Evaluate the model\n",
    "  eval_results = trainer.evaluate()\n",
    "  \n",
    "  return eval_results, training_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'learning_rate': 1e-06,\n",
    "'num_train_epochs': 1,\n",
    "'per_device_train_batch_size': 1,\n",
    "'optim': 'adafactor',\n",
    "'num_iterations': 1,\n",
    "'max_steps':3,\n",
    "'gradient_accumulation_steps':2}\n",
    "eval_results, training_output =train_model(hyperparameters)\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "        best_hyperparameters = None\n",
    "        best_loss = float('inf')\n",
    "        # Define hyperparameter search space\n",
    "        hyperparameter_space = {\n",
    "        \"learning_rate\": [1e-6, 1e-5, 1e-4],\n",
    "        \"num_train_epochs\": [1,5,10,20],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "        \"num_iterations\": [1],\n",
    "        \"max_steps\": [3],\n",
    "        \"gradient_accumulation_steps\": [3],\n",
    "        }\n",
    "        # Generate all combinations of hyperparameters\n",
    "        all_hyperparameters = list(itertools.product(*hyperparameter_space.values()))\n",
    "\n",
    "        # Assuming all_hyperparameters is a list of hyperparameter combinations\n",
    "        for hyperparameter_values in tqdm(all_hyperparameters):\n",
    "            hyperparameters = dict(zip(hyperparameter_space.keys(), hyperparameter_values))\n",
    "            \n",
    "            # Evaluate the model\n",
    "            # Print the current hyperparameters\n",
    "            print(\"Using hyperparameters:\")\n",
    "            for key, value in hyperparameters.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            eval_results, training_output = train_model(hyperparameters)\n",
    "            clear_output()\n",
    "            # Check if this set of hyperparameters gives better results\n",
    "            if eval_results[\"eval_loss\"] < best_loss:\n",
    "                    best_loss = eval_results[\"eval_loss\"]\n",
    "                    best_hyperparameters = hyperparameters\n",
    "\n",
    "        return best_hyperparameters, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to find the best hyperparameters\n",
    "best_hyperparameters, best_loss = find_best_hyperparameters()\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
