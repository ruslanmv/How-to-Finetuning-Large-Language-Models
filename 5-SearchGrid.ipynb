{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0829611d00a141089d5dde8bee8abda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Blog\\How-to-Finetuning-Large-Language-Models\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\066226758\\.cache\\huggingface\\hub\\models--EleutherAI--pythia-70m-deduped. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0791dcb2330c4b5899eca1526d6f9c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12e25a9e494475a9d0821b39136a85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:01,555 - DEBUG - utilities - Config: datasets.path: c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n",
      "datasets.use_hf: false\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m-deduped\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize False c:\\Blog\\How-to-Finetuning-Large-Language-Models\\content\\ai-medical-chatbot_processed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:01,931 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-f1c6af33428df321/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n",
      "2024-04-10 14:42:01,945 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-f1c6af33428df321/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc64efb4592a4f8f803447d249bc9976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2024-04-10 14:42:02,040 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-f1c6af33428df321/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/tmp4ae1ycbg\n",
      "2024-04-10 14:42:04,738 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-f1c6af33428df321/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/tmpynrhkjug\n",
      "2024-04-10 14:42:04,762 - DEBUG - fsspec.local - open file: C:/Users/066226758/.cache/huggingface/datasets/json/default-f1c6af33428df321/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/tmpjsdsk3lr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777fc3ce6dc49f38e47c794e661decd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfd9c2cc49448809c0c280945d4de2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:38,571 - DEBUG - utilities - Select CPU device\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"EleutherAI/pythia-160m\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "#model_name = \"EleutherAI/pythia-70m\"\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "# Join the folder path\n",
    "folder_path = os.path.join(current_directory, \"content\")\n",
    "dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "dataset_path = os.path.join(folder_path, dataset_name)\n",
    "#dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hyperparameters):\n",
    "  max_steps = hyperparameters[\"max_steps\"]\n",
    "  trained_model_name = f\"ai_medical_{max_steps}_steps\"\n",
    "  output_dir = trained_model_name\n",
    "  training_args = TrainingArguments(\n",
    "    # Learning rate\n",
    "    learning_rate=hyperparameters[\"learning_rate\"],\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_train_epochs=hyperparameters[\"num_train_epochs\"],\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overrides num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=hyperparameters[\"per_device_train_batch_size\"],\n",
    "\n",
    "    # Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # Other arguments\n",
    "    overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "    disable_tqdm=False, # Disable progress bars\n",
    "    eval_steps=120, # Number of update steps between two evaluations\n",
    "    save_steps=120, # After # steps model is saved\n",
    "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=hyperparameters[\"optim\"],\n",
    "    gradient_accumulation_steps = hyperparameters['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=False,\n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    "  )\n",
    "  base_model.to(device)\n",
    "  model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "      {\n",
    "        \"input_ids\": torch.zeros(\n",
    "            (1, training_config[\"model\"][\"max_length\"])\n",
    "        )\n",
    "      }\n",
    "    )\n",
    "    * training_args.gradient_accumulation_steps\n",
    "  )\n",
    "\n",
    "  #print(base_model)\n",
    "  print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "  print(\"Flops\", model_flops / 1e9, \"GFLOPs\")\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "  training_output = trainer.train()\n",
    "  # Evaluate the model\n",
    "  eval_results = trainer.evaluate()\n",
    "  \n",
    "  return eval_results, training_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint 0.30687256 GB\n",
      "Flops 1097.833906176 GFLOPs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9039f9627b2244ed8faeaac20e5e6c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:40,807 - DEBUG - utilities - Step (1) Logs: {'loss': 5.0693, 'learning_rate': 1e-06, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0693, 'learning_rate': 1e-06, 'epoch': 0.0, 'iter_time': 0.0, 'flops': 0.0, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:41,984 - DEBUG - utilities - Step (2) Logs: {'loss': 5.0176, 'learning_rate': 5e-07, 'epoch': 0.0, 'iter_time': 1.176931619644165, 'flops': 932793280299.4285, 'remaining_time': 1.176931619644165}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.0176, 'learning_rate': 5e-07, 'epoch': 0.0, 'iter_time': 1.176931619644165, 'flops': 932793280299.4285, 'remaining_time': 1.176931619644165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:43,130 - DEBUG - utilities - Step (3) Logs: {'loss': 5.3909, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 1.1616612672805786, 'flops': 945055100912.5088, 'remaining_time': 0.0}\n",
      "2024-04-10 14:42:43,133 - DEBUG - utilities - Step (3) Logs: {'train_runtime': 3.8223, 'train_samples_per_second': 1.57, 'train_steps_per_second': 0.785, 'total_flos': 91932868608.0, 'train_loss': 5.159242312113444, 'epoch': 0.01, 'iter_time': 1.1631618738174438, 'flops': 943835876061.6521, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3909, 'learning_rate': 0.0, 'epoch': 0.01, 'iter_time': 1.1616612672805786, 'flops': 945055100912.5088, 'remaining_time': 0.0}\n",
      "{'train_runtime': 3.8223, 'train_samples_per_second': 1.57, 'train_steps_per_second': 0.785, 'train_loss': 5.159242312113444, 'epoch': 0.01, 'iter_time': 1.1631618738174438, 'flops': 943835876061.6521, 'remaining_time': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f37ebf0fc404720a59f2f081b9310f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:42:52,253 - DEBUG - utilities - Step (3) Logs: {'eval_loss': 4.671843528747559, 'eval_runtime': 9.1069, 'eval_samples_per_second': 10.981, 'eval_steps_per_second': 10.981, 'epoch': 0.01, 'iter_time': 5.72236967086792, 'flops': 191849525514.74884, 'remaining_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters={'learning_rate': 1e-06,\n",
    "'num_train_epochs': 1,\n",
    "'per_device_train_batch_size': 1,\n",
    "'optim': 'adafactor',\n",
    "'num_iterations': 1,\n",
    "'max_steps':3,\n",
    "'gradient_accumulation_steps':2}\n",
    "eval_results, training_output =train_model(hyperparameters)\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters():\n",
    "        best_hyperparameters = None\n",
    "        best_loss = float('inf')\n",
    "        # Define hyperparameter search space\n",
    "        hyperparameter_space = {\n",
    "        \"learning_rate\": [6.0e-5, 3.0e-4],\n",
    "        \"num_train_epochs\": [1,5],\n",
    "        \"per_device_train_batch_size\": [1],\n",
    "        \"optim\": [\"adafactor\"],\n",
    "        \"num_iterations\": [1],\n",
    "        \"max_steps\": [1,5,10,50],\n",
    "        \"gradient_accumulation_steps\": [2],\n",
    "        }\n",
    "        # Generate all combinations of hyperparameters\n",
    "        all_hyperparameters = list(itertools.product(*hyperparameter_space.values()))\n",
    "\n",
    "        # Assuming all_hyperparameters is a list of hyperparameter combinations\n",
    "        for hyperparameter_values in tqdm(all_hyperparameters):\n",
    "            hyperparameters = dict(zip(hyperparameter_space.keys(), hyperparameter_values))\n",
    "            \n",
    "            # Evaluate the model\n",
    "            # Print the current hyperparameters\n",
    "            print(\"Using hyperparameters:\")\n",
    "            for key, value in hyperparameters.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            eval_results, training_output = train_model(hyperparameters)\n",
    "            clear_output()\n",
    "            # Check if this set of hyperparameters gives better results\n",
    "            if eval_results[\"eval_loss\"] < best_loss:\n",
    "                    best_loss = eval_results[\"eval_loss\"]\n",
    "                    best_hyperparameters = hyperparameters\n",
    "\n",
    "        return best_hyperparameters, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [07:50<00:00, 29.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 6e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 1, 'optim': 'adafactor', 'num_iterations': 1, 'max_steps': 50, 'gradient_accumulation_steps': 2}\n",
      "Best loss: 2.6349027156829834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to find the best hyperparameters\n",
    "best_hyperparameters, best_loss = find_best_hyperparameters()\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
