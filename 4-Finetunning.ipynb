{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Large Language Models\n",
    "In this notebook we are going to fine tune a simple Large Model to create a custom doctor chatbot assitant.\n",
    "This notebook collect the three passages needed\n",
    "1. Data Preparation\n",
    "2. Traning\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "train_data = dataset[\"train\"]\n",
    "# For this demo, let's choose the first 1000 dialogues\n",
    "df = pd.DataFrame(train_data[:1000])\n",
    "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"question\", \"Doctor\": \"answer\"})\n",
    "# Clean the question and answer columns\n",
    "df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named 'df' and the column is named 'df' and the column is named 'question'\n",
    "df['question'] = df['question'].str.lstrip('Q. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Various ways of formatting your data\n",
    "\n",
    "examples = df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"{question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "\n",
    "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
    "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "# Join the folder path\n",
    "folder_path = os.path.join(current_directory, \"content\")\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dataset path\n",
    "dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "dataset_path = os.path.join(folder_path, dataset_name)\n",
    "\n",
    "### Common ways of storing your data\n",
    "with jsonlines.open(dataset_path, 'w') as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from llama import BasicModelRunner\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the AI Medical Chatbot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "# Join the folder path\n",
    "folder_path = os.path.join(current_directory, \"content\")\n",
    "dataset_name = \"ai-medical-chatbot_processed.jsonl\"\n",
    "dataset_path = os.path.join(folder_path, dataset_name)\n",
    "#dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"ruslanmv/ai-medical-chatbot\"\n",
    "#use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to carry out inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the previos inference function doesn't explicitly set the attention_mask and pad_token_id arguments in the model.generate call. These are crucial for the model to understand how to process the input and generate a proper response.\n",
    "\n",
    "Here's how to fix the issue:\n",
    "\n",
    "Setting pad_token_id:\n",
    "\n",
    "Access the eos_token_id (end-of-sentence token) from your tokenizer. This value usually indicates the pad token for the model.\n",
    "\n",
    "In the model.generate call, add the argument pad_token_id=tokenizer.eos_token_id. This tells the model which token represents padding.\n",
    "Setting attention_mask:\n",
    "\n",
    "The attention_mask informs the model which parts of the input sequence to focus on during generation. It's a binary mask where 1 indicates a valid token and 0 indicates padding.\n",
    "For your code, you can create the attention_mask directly:\n",
    "\n",
    "```Python\n",
    "attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "attention_mask[input_ids[:, 1:] == tokenizer.pad_token_id] = 0  # Set padding to 0\n",
    "```\n",
    "\n",
    "This code snippet creates a mask with all 1s (meaning all tokens are considered) and then sets the mask elements corresponding to padding tokens (identified by tokenizer.pad_token_id) to 0 (ignored by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_attempt(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "      text,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "  attention_mask[input_ids[:, 1:] == tokenizer.pad_token_id] = 0  # Set padding to 0\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "      input_ids.to(device),\n",
    "      max_length=max_output_tokens,\n",
    "      attention_mask=attention_mask,\n",
    "      pad_token_id=tokenizer.eos_token_id  # Set pad token\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there are stills some points to consider\n",
    "\n",
    "- Masking Purpose: The attention mask indicates which input tokens should be attended to during generation. Masking padding tokens ensure they don't influence the model's attention.\n",
    "\n",
    "- Crucial First Element: The CLS token (index 0) is essential for tasks like question-answering, so it must be included in the masking.\n",
    "\n",
    "\n",
    "- Clearer Slicing: While [:, 1:] would work if corrected to include the first element, using `input_ids == tokenizer.pad_token_id` is more explicit and less prone to errors.\n",
    "Consider torch.where: For more complex masking scenarios, explore torch.where for conditional assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_new(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "      text,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "\n",
    "  # Fix: Mask all padding tokens, including the first element\n",
    "  attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "      input_ids.to(device),\n",
    "      max_length=max_output_tokens,\n",
    "      attention_mask=attention_mask,\n",
    "      pad_token_id=tokenizer.eos_token_id  # Set pad token\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "  return generated_text_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_dataset[1]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from ai-medical-chatbot: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference_new(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"ai_medical_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate= 5.0e-5 ,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=10,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 2,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a few steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerCallback\n",
    "class MetricsCollector(TrainerCallback):\n",
    "  \"\"\"\n",
    "  Callback to collect metrics during training.\n",
    "\n",
    "  This callback stores all the logs it receives during training in a list\n",
    "  called `metrics`. This list can then be used to plot training loss, learning rate,\n",
    "  and other metrics.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.metrics = []\n",
    "\n",
    "  def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Stores the logs received during training.\n",
    "\n",
    "    This method is called whenever the trainer logs information. It simply\n",
    "    appends the entire `logs` dictionary to the `metrics` list.\n",
    "\n",
    "    Args:\n",
    "      args: Arguments passed to the trainer.\n",
    "      state: State of the trainer.\n",
    "      control: Control object for the trainer.\n",
    "      logs: Dictionary containing the logged metrics. (optional)\n",
    "      **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"Available logs:\", logs)  # Print the logs dictionary to see its keys for debugging\n",
    "\n",
    "    self.metrics.append(logs)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(metrics):\n",
    "  \"\"\"\n",
    "  Plots the training loss from the collected metrics.\n",
    "\n",
    "  This function iterates through the `metrics` list and extracts the `loss` value\n",
    "  from each dictionary. It then filters out any entries where `loss` is missing\n",
    "  and plots the remaining values.\n",
    "\n",
    "  Args:\n",
    "    metrics: List of dictionaries containing training logs.\n",
    "  \"\"\"\n",
    "\n",
    "  losses = [m.get('loss', None) for m in metrics]  # Use .get() to handle missing keys\n",
    "  non_none_losses = [loss for loss in losses if loss is not None]\n",
    "  plt.plot(non_none_losses)\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training Loss')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_rate(metrics):\n",
    "  \"\"\"\n",
    "  Plots the learning rate from the collected metrics.\n",
    "\n",
    "  This function follows the same logic as `plot_loss` to extract and plot the\n",
    "  learning rate values from the `metrics` list, handling missing entries.\n",
    "\n",
    "  Args:\n",
    "    metrics: List of dictionaries containing training logs.\n",
    "  \"\"\"\n",
    "\n",
    "  learning_rates = [m.get('learning_rate', None) for m in metrics]\n",
    "  non_none_learning_rates = [lr for lr in learning_rates if lr is not None]\n",
    "  plt.plot(non_none_learning_rates)\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel('Learning Rate')\n",
    "  plt.title('Learning Rate')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_collector = MetricsCollector()\n",
    "trainer.add_callback(metrics_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(metrics_collector.metrics)\n",
    "plot_learning_rate(metrics_collector.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss vs Iteration: Ideal Shape and Expectations\n",
    "\n",
    "**Desired Shape:**\n",
    "\n",
    "The ideal loss curve for training depicts a **downward trend**, indicating the model's improvement in minimizing the loss function over iterations. This signifies the model is learning the patterns in the data and making better predictions.\n",
    "\n",
    "**Expected Behavior:**\n",
    "\n",
    "* **Initial Rapid Decrease:** Early iterations often show a steep decline as the model grasps basic patterns.\n",
    "* **Gradual Descent:** As the model encounters more complex relationships, the decrease becomes slower and steadier.\n",
    "* **Plateau (Optional):** In some cases,  the loss may reach a plateau, indicating the model has achieved its optimal performance with the current hyperparameters.\n",
    "\n",
    "**Warning Signs:**\n",
    "\n",
    "* **Stagnation:** If the loss remains flat from the beginning, the model might not be learning effectively. This could be due to factors like insufficient data, inappropriate model architecture, or high learning rate.\n",
    "* **Oscillation:** Erratic fluctuations can indicate the model is overfitting to the training data. Techniques like regularization or reducing learning rate can help.\n",
    "\n",
    "## Learning Rate vs Iteration: Ideal Behavior and Expectations\n",
    "\n",
    "**Desired Behavior:**\n",
    "\n",
    "The learning rate can be adjusted throughout training using a learning rate schedule. Here are some common approaches:\n",
    "\n",
    "* **Constant Learning Rate:** A simple approach, but may not be optimal for all scenarios.\n",
    "* **Step Decay:** Learning rate is reduced by a fixed amount at specific intervals (epochs).\n",
    "* **Exponential Decay:** Learning rate decreases exponentially over time.\n",
    "* **Warm Restarts:** Learning rate is periodically increased to a higher value, allowing the model to escape local minima.\n",
    "\n",
    "**General Expectations:**\n",
    "\n",
    "* The learning rate should be high enough for the model to learn effectively initially.\n",
    "* A gradual decrease in learning rate helps the model converge to a minimum more precisely.\n",
    "* Choosing the right learning rate schedule depends on the specific problem and dataset.\n",
    "\n",
    "**Warning Signs:**\n",
    "\n",
    "* **Too High Learning Rate:** Can lead to large oscillations or divergence in the loss curve.\n",
    "* **Too Low Learning Rate:** Slows down training significantly and may prevent the model from reaching optimal performance.\n",
    "\n",
    "By monitoring both loss and learning rate curves, you can gain valuable insights into the training process and adjust hyperparameters for better results.\n",
    "\n",
    "**Additional Tips:**\n",
    "\n",
    "* Use validation loss to track generalization performance and avoid overfitting.\n",
    "* Experiment with different learning rate schedules to find the best fit for your problem.\n",
    "* Consider techniques like early stopping to prevent overfitting when the validation loss starts to increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model.to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = test_dataset[1]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference_new(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run same model trained for two epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"ruslanmv/ai-medical-chatbot-finetuned\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"ruslanmv/ai-medical-chatbot-finetuned\")\n",
    "#finetuned_longer_model.to(device)\n",
    "#print(\"Finetuned longer model's answer: \")\n",
    "#print(inference_new(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore moderation using small model\n",
    "First, try the non-finetuned base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference_new(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try moderation with finetuned small model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inference_new(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))\n",
    "print(inference_new(\"What do you think of Mars?\", finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming test_dataset is already loaded\n",
    "test_questions = [test_dataset[i]['question'] for i in range(len(test_dataset))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_questions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the base model and tokenizer\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the model's answer\n",
    "def get_answer(question, model, tokenizer):\n",
    "    return inference_new(question, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the test questions and generate answers using both models\n",
    "results = []\n",
    "for question in tqdm(test_questions[:10]): #test_questions:\n",
    "    finetuned_answer = get_answer(question, finetuned_slightly_model, tokenizer)\n",
    "    base_answer = get_answer(question, base_model, base_tokenizer)\n",
    "    result = {'question': question, 'trained model': finetuned_answer, 'Base Model': base_answer}\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "\n",
    "# Style the DataFrame for better readability\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "\n",
    "# Display the DataFrame\n",
    "style_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
