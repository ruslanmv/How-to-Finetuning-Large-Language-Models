{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Generate predicted answer\n",
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "    # Fix: Mask all padding tokens, including the first element\n",
    "    attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=max_output_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad token\n",
    "    )\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "    return generated_text_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The French are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes\n"
     ]
    }
   ],
   "source": [
    "# Sample question and answer from evaluation dataset\n",
    "test_question = \"What is the capital of France?\"\n",
    "answer = \"Paris\"\n",
    "predicted_answer = inference(test_question, model, tokenizer)\n",
    "print(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(answer, predicted_answer):\n",
    "  \"\"\"\n",
    "  This function calculates the exact match ratio between the answer and predicted answer.\n",
    "\n",
    "  Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "  Returns:\n",
    "      A float value (1.0 for exact match, 0.0 otherwise).\n",
    "  \"\"\"\n",
    "  return 1.0 if answer.lower() == predicted_answer.lower() else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(answer, predicted_answer):\n",
    "  \"\"\"\n",
    "  This function calculates a BLEU score between the answer and predicted answer using the `nltk` library with smoothing.\n",
    "\n",
    "  Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "  Returns:\n",
    "      A float value representing the BLEU score (higher is better).\n",
    "\n",
    "  **Requires `nltk` library to be installed (`pip install nltk`).**\n",
    "  \"\"\"\n",
    "  from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "  reference = [answer.split()]\n",
    "  candidate = predicted_answer.split()\n",
    "  smooth = SmoothingFunction()  # Create a SmoothingFunction object\n",
    "  return sentence_bleu(reference, candidate, smoothing_function=smooth.method0)  # Use method0 from SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(answer, predicted_answer, n):\n",
    "  \"\"\"\n",
    "  This function calculates ROUGE-N score (e.g., ROUGE-1, ROUGE-2) between the answer and predicted answer using the `datasets` library.\n",
    "\n",
    "  Args:\n",
    "    answer: The ground truth answer (string).\n",
    "    predicted_answer: The predicted answer by the LLM (string).\n",
    "    n: The n-gram size for the ROUGE metric (e.g., 1 for unigrams).\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing precision, recall, and F1 score for ROUGE-N.\n",
    "\n",
    "  **Requires `datasets` library to be installed (`pip install datasets`).**\n",
    "  \"\"\"\n",
    "  from datasets import load_metric\n",
    "  rouge = load_metric(\"rouge\")\n",
    "\n",
    "  if n == 1:\n",
    "    return rouge.compute(predictions=[predicted_answer], references=[[answer]], rouge_types=[\"rouge1\"])\n",
    "  elif n == 2:\n",
    "    return rouge.compute(predictions=[predicted_answer], references=[[answer]], rouge_types=[\"rouge2\"])\n",
    "  # You can add similar logic for ROUGE-L or other variants\n",
    "  else:\n",
    "    raise ValueError(\"ROUGE-N not supported for n > 2. Choose n=1 or n=2.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_a(answer, predicted_answer):\n",
    "    \"\"\"\n",
    "    This function calculates F1 score between the answer and predicted answer \n",
    "\n",
    "    Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "    Returns:\n",
    "      A float value representing the F1 score (higher is better).\n",
    "    \"\"\"\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = len(answer_tokens.intersection(predicted_tokens)) / len(predicted_tokens)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(answer_tokens.intersection(predicted_tokens)) / len(answer_tokens)\n",
    "    \n",
    "    # Handle division by zero for precision or recall\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(answer, predicted_answer):\n",
    "    \"\"\"\n",
    "    This function calculates F1 score between the answer and predicted answer \n",
    "\n",
    "    Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "    Returns:\n",
    "      A float value representing the F1 score (higher is better).\n",
    "    \"\"\"\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = len(answer_tokens.intersection(predicted_tokens)) / len(predicted_tokens)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(answer_tokens.intersection(predicted_tokens)) / len(answer_tokens)\n",
    "    \n",
    "    # Handle division by zero for precision or recall\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Blog\\How-to-Finetuning-Large-Language-Models\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Blog\\How-to-Finetuning-Large-Language-Models\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Blog\\How-to-Finetuning-Large-Language-Models\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the capital of France?\n",
      "Predicted Answer: The capital is Paris\n",
      "Ground-Truth Answer: Paris\n",
      "BLEU Score: 1.2882297539194154e-231\n",
      "BLEU Score: 1.2882297539194154e-231\n",
      "ROUGE-1 Score: {'rouge1': AggregateScore(low=Score(precision=0.25, recall=1.0, fmeasure=0.4), mid=Score(precision=0.25, recall=1.0, fmeasure=0.4), high=Score(precision=0.25, recall=1.0, fmeasure=0.4))}\n",
      "ROUGE-2 Score: {'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "F1 Score: 0.4\n",
      "F1 Score: 0.4\n",
      "Exact Match Ratio: 0.0\n",
      "exact_match_value 0\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "test_question = \"What is the capital of France?\"\n",
    "answer = \"Paris\"\n",
    "predicted_answer = 'The French are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes.'\n",
    "predicted_answer = \"The capital is Paris\"\n",
    "# Calculate BLEU Score\n",
    "bleu = sentence_bleu([answer.split()], predicted_answer.split())\n",
    "# Calculate Exact Match\n",
    "exact_match_value = int(predicted_answer.strip() == answer.strip())\n",
    "\n",
    "print(f\"Test Question: {test_question}\")\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n",
    "print(f\"Ground-Truth Answer: {answer}\")\n",
    "print(f\"BLEU Score: {bleu}\")\n",
    "print(\"BLEU Score:\", bleu_score(answer, predicted_answer))\n",
    "print(\"ROUGE-1 Score:\", rouge_n(answer, predicted_answer, 1))\n",
    "# You can call rouge_n with n=2 for ROUGE-2 score\n",
    "print(\"ROUGE-2 Score:\", rouge_n(answer, predicted_answer, 2))\n",
    "print(\"F1 Score:\", f1_score_a(answer, predicted_answer))\n",
    "print(\"F1 Score:\", f1_score(answer, predicted_answer))\n",
    "print(\"Exact Match Ratio:\", exact_match(answer, predicted_answer))\n",
    "print(\"exact_match_value\",exact_match_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
