{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Generate predicted answer\n",
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    attention_mask = torch.ones_like(input_ids)  # Create mask with all 1s\n",
    "    # Fix: Mask all padding tokens, including the first element\n",
    "    attention_mask[input_ids == tokenizer.pad_token_id] = 0\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids.to(device),\n",
    "        max_length=max_output_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Set pad token\n",
    "    )\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "    return generated_text_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The French are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes\n"
     ]
    }
   ],
   "source": [
    "# Sample question and answer from evaluation dataset\n",
    "test_question = \"What is the capital of France?\"\n",
    "answer = \"Paris\"\n",
    "predicted_answer = inference(test_question, model, tokenizer)\n",
    "print(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(answer, predicted_answer):\n",
    "  \"\"\"\n",
    "  This function calculates the exact match ratio between the answer and predicted answer.\n",
    "\n",
    "  Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "  Returns:\n",
    "      A float value (1.0 for exact match, 0.0 otherwise).\n",
    "  \"\"\"\n",
    "  return 1.0 if answer.lower() == predicted_answer.lower() else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bleu_score(answer, predicted_answer):\n",
    "  \"\"\"\n",
    "  This function calculates a BLEU score between the answer and predicted answer using the `nltk` library with smoothing.\n",
    "\n",
    "  Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "  Returns:\n",
    "      A float value representing the BLEU score (higher is better).\n",
    "\n",
    "  **Requires `nltk` library to be installed (`pip install nltk`).**\n",
    "  \"\"\"\n",
    "  from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "  reference = [answer.split()]\n",
    "  candidate = predicted_answer.split()\n",
    "  smooth = SmoothingFunction()  # Create a SmoothingFunction object\n",
    "  return sentence_bleu(reference, candidate, smoothing_function=smooth.method0)  # Use method0 from SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(answer, predicted_answer, n=2):\n",
    "    \"\"\"\n",
    "    This function calculates a BLEU score between the answer and predicted answer using the `nltk` library with smoothing.\n",
    "\n",
    "    Args:\n",
    "        answer: The ground truth answer (string).\n",
    "        predicted_answer: The predicted answer by the LLM (string).\n",
    "        n: The n-gram order (default is 2 for bigrams).\n",
    "\n",
    "    Returns:\n",
    "        A float value representing the BLEU score (higher is better).\n",
    "\n",
    "    **Requires `nltk` library to be installed (`pip install nltk`).**\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    reference = [answer.split()]\n",
    "    candidate = predicted_answer.split()\n",
    "    smooth = SmoothingFunction()  # Create a SmoothingFunction object\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smooth.method1, weights=(1/n,)*n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(answer, predicted_answer, n):\n",
    "  \"\"\"\n",
    "  This function calculates ROUGE-N score (e.g., ROUGE-1, ROUGE-2) between the answer and predicted answer using the `datasets` library.\n",
    "\n",
    "  Args:\n",
    "    answer: The ground truth answer (string).\n",
    "    predicted_answer: The predicted answer by the LLM (string).\n",
    "    n: The n-gram size for the ROUGE metric (e.g., 1 for unigrams).\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing precision, recall, and F1 score for ROUGE-N.\n",
    "\n",
    "  **Requires `datasets` library to be installed (`pip install datasets`).**\n",
    "  \"\"\"\n",
    "  from datasets import load_metric\n",
    "  rouge = load_metric(\"rouge\")\n",
    "\n",
    "  if n == 1:\n",
    "    return rouge.compute(predictions=[predicted_answer], references=[[answer]], rouge_types=[\"rouge1\"])\n",
    "  elif n == 2:\n",
    "    return rouge.compute(predictions=[predicted_answer], references=[[answer]], rouge_types=[\"rouge2\"])\n",
    "  # You can add similar logic for ROUGE-L or other variants\n",
    "  else:\n",
    "    raise ValueError(\"ROUGE-N not supported for n > 2. Choose n=1 or n=2.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_a(answer, predicted_answer):\n",
    "    \"\"\"\n",
    "    This function calculates F1 score between the answer and predicted answer \n",
    "\n",
    "    Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "    Returns:\n",
    "      A float value representing the F1 score (higher is better).\n",
    "    \"\"\"\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = len(answer_tokens.intersection(predicted_tokens)) / len(predicted_tokens)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(answer_tokens.intersection(predicted_tokens)) / len(answer_tokens)\n",
    "    \n",
    "    # Handle division by zero for precision or recall\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(answer, predicted_answer):\n",
    "    \"\"\"\n",
    "    This function calculates F1 score between the answer and predicted answer \n",
    "\n",
    "    Args:\n",
    "      answer: The ground truth answer (string).\n",
    "      predicted_answer: The predicted answer by the LLM (string).\n",
    "\n",
    "    Returns:\n",
    "      A float value representing the F1 score (higher is better).\n",
    "    \"\"\"\n",
    "    answer_tokens = set(answer.lower().split())\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = len(answer_tokens.intersection(predicted_tokens)) / len(predicted_tokens)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(answer_tokens.intersection(predicted_tokens)) / len(answer_tokens)\n",
    "    \n",
    "    # Handle division by zero for precision or recall\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def cosine_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two strings using the Bag-of-Words model.\n",
    "\n",
    "    Args:\n",
    "        str1: The first string.\n",
    "        str2: The second string.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the cosine similarity between the two strings.\n",
    "    \"\"\"\n",
    "    # Tokenize the strings\n",
    "    tokens1 = str1.split()\n",
    "    tokens2 = str2.split()\n",
    "\n",
    "    # Create bag of words for each string\n",
    "    bow1 = Counter(tokens1)\n",
    "    bow2 = Counter(tokens2)\n",
    "\n",
    "    # Get the set of all unique words\n",
    "    all_words = set(bow1.keys()).union(set(bow2.keys()))\n",
    "\n",
    "    # Compute dot product\n",
    "    dot_product = sum(bow1[word] * bow2[word] for word in all_words)\n",
    "\n",
    "    # Compute magnitudes\n",
    "    magnitude1 = math.sqrt(sum(bow1[word] ** 2 for word in all_words))\n",
    "    magnitude2 = math.sqrt(sum(bow2[word] ** 2 for word in all_words))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product / (magnitude1 * magnitude2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the cosine similarity between two strings using the Bag-of-Words model. It tokenizes the strings, creates bag of words for each string, computes the dot product, and then computes the magnitudes of the vectors. Finally, it calculates the cosine similarity using the dot product and magnitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What is the capital of France?\n",
      "Predicted Answer:  The capital Paris\n",
      "Ground-Truth Answer: The capital is Paris\n",
      "BLEU Score: 0.5066641486392106\n",
      "ROUGE-1 Score: {'rouge1': AggregateScore(low=Score(precision=1.0, recall=0.75, fmeasure=0.8571428571428571), mid=Score(precision=1.0, recall=0.75, fmeasure=0.8571428571428571), high=Score(precision=1.0, recall=0.75, fmeasure=0.8571428571428571))}\n",
      "ROUGE-2 Score: {'rouge2': AggregateScore(low=Score(precision=0.5, recall=0.3333333333333333, fmeasure=0.4), mid=Score(precision=0.5, recall=0.3333333333333333, fmeasure=0.4), high=Score(precision=0.5, recall=0.3333333333333333, fmeasure=0.4))}\n",
      "F1 Score: 0.8571428571428571\n",
      "F1 Score: 0.8571428571428571\n",
      "Cosine Similarity: 0.8660254037844387\n",
      "Exact Match Ratio: 0.0\n",
      "exact_match_value 0\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "test_question = \"What is the capital of France?\"\n",
    "answer = \"The capital is Paris\"\n",
    "predicted_answer = 'The French are the only ones who can afford to pay their taxes. They are the only ones who can afford to pay their taxes.'\n",
    "predicted_answer = \" The capital Paris\"\n",
    "# Calculate Exact Match\n",
    "exact_match_value = int(predicted_answer.strip() == answer.strip())\n",
    "# Calculate BLEU Score\n",
    "#bleu = sentence_bleu([answer.split()], predicted_answer.split())\n",
    "print(f\"Test Question: {test_question}\")\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n",
    "print(f\"Ground-Truth Answer: {answer}\")\n",
    "print(\"BLEU Score:\", bleu_score(answer, predicted_answer))\n",
    "print(\"ROUGE-1 Score:\", rouge_n(answer, predicted_answer, 1))\n",
    "# You can call rouge_n with n=2 for ROUGE-2 score\n",
    "print(\"ROUGE-2 Score:\", rouge_n(answer, predicted_answer, 2))\n",
    "print(\"F1 Score:\", f1_score_a(answer, predicted_answer))\n",
    "print(\"F1 Score:\", f1_score(answer, predicted_answer))\n",
    "print(\"Cosine Similarity:\", cosine_similarity(answer, predicted_answer))\n",
    "print(\"Exact Match Ratio:\", exact_match(answer, predicted_answer))\n",
    "print(\"exact_match_value\",exact_match_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.7090416310250969\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "answer = \"The cat sat on the mat\"\n",
    "predicted_answer = \"The cat on the mat\"\n",
    "score = bleu_score(answer, predicted_answer, n=2)\n",
    "print(\"BLEU Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8164965809277259\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def cosine_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two strings using the Bag-of-Words model.\n",
    "\n",
    "    Args:\n",
    "        str1: The first string.\n",
    "        str2: The second string.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the cosine similarity between the two strings.\n",
    "    \"\"\"\n",
    "    # Tokenize the strings\n",
    "    tokens1 = str1.split()\n",
    "    tokens2 = str2.split()\n",
    "\n",
    "    # Create bag of words for each string\n",
    "    bow1 = Counter(tokens1)\n",
    "    bow2 = Counter(tokens2)\n",
    "\n",
    "    # Get the set of all unique words\n",
    "    all_words = set(bow1.keys()).union(set(bow2.keys()))\n",
    "\n",
    "    # Compute dot product\n",
    "    dot_product = sum(bow1[word] * bow2[word] for word in all_words)\n",
    "\n",
    "    # Compute magnitudes\n",
    "    magnitude1 = math.sqrt(sum(bow1[word] ** 2 for word in all_words))\n",
    "    magnitude2 = math.sqrt(sum(bow2[word] ** 2 for word in all_words))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Example usage:\n",
    "str1 = \"apple banana mango\"\n",
    "str2 = \"banana orange\"\n",
    "str2 = \"apple banana\"\n",
    "similarity = cosine_similarity(str1, str2)\n",
    "print(\"Cosine Similarity:\", similarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
